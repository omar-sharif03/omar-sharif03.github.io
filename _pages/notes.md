---
title: "My Notes"
excerpt: ""
author_profile: true
permalink: /notes/
redirect_from: 
  - /about.html
---
My hand notes while I completed the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a> on Coursera.
 * Neural Networks and Deep Learning <a href="https://drive.google.com/file/d/1P6iF2SFhkT9jWwTU431mN644HGYcQdpu/view?usp=sharing">Note</a>.
 * Improving Deep Neural Networks  <a href="https://drive.google.com/file/d/1P6iF2SFhkT9jWwTU431mN644HGYcQdpu/view?usp=sharing">Note</a>.
 * Structuring Machine Learning Projects <a href="https://drive.google.com/file/d/1P6iF2SFhkT9jWwTU431mN644HGYcQdpu/view?usp=sharing">Note</a>.
 * Convolutional Neural Networks <a href="https://drive.google.com/file/d/1bCqlppOAW270Q4ZEv3lI6uw0-Zs1BI23/view?usp=sharing">Note</a>.
 * Sequence Models <a href="https://drive.google.com/file/d/14o0ptgBZw8sdzFXg35NqEL5Ar8KrN3_L/view?usp=sharing">Note</a>.

## <font color="#00cc66"> NLP Papers </font>
I found these papers useful in clarifying my understanding of various NLP topics. 
<pre>
* <span style="color:rgb(201, 76, 76)"><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a></span> 
* <span style="color:rgb(201, 76, 76)"><a href="https://arxiv.org/abs/1301.3781">[Word2Vec]</a> <a href="https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html">[Negative Sampling]</a> <a href="https://aclanthology.org/D14-1162.pdf">[GloVe]</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://aclanthology.org/D14-1179.pdf">Learning Phrase Representations using RNN Encoderâ€“Decoder
for Statistical Machine Translation</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">Sequence to Sequence Learning with Neural Networks</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate (Paper that introduces Attention)</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a></span>
* <span style="color:rgb(201, 76, 76)"><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a></span>
* <span style="color:rgb(201, 76, 76)"><a href="https://aclanthology.org/N18-1202/">[ELMo]</a> <a href="https://arxiv.org/pdf/1810.04805.pdf">[BERT]</a> <a href="https://arxiv.org/abs/1907.11692">[RoBERTa]</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://arxiv.org/abs/2003.10555">[ELECTRA]</a> <a href="https://arxiv.org/abs/1909.11942">[ALBERT]</a> <a href="https://arxiv.org/abs/1906.08237">[XLNet]</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://arxiv.org/abs/2003.10555">[ELECTRA]</a> <a href="https://arxiv.org/abs/1909.11942">[ALBERT]</a> <a href="https://arxiv.org/abs/1906.08237">[XLNet]</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">[GPT]</a> <a href="https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf">[GPT-2]</a> <a href="https://arxiv.org/abs/2005.14165">[GPT-3]</a> </span>
</pre>


## <font color="#00cc66"> Useful Links </font>
Few blog posts/links that I found really useful to understand various fundamental concepts of NLP.
<pre>
* <span style="color:rgb(201, 76, 76)">Andrej Karpathy's coding-based backpropagation post</span> <a href="http://karpathy.github.io/neuralnets/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Andrej Karpathy's blog on RNNs</span> <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Understanding LSTM Networks</span> <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The Illustrated Word2vec</span> <a href="https://jalammar.github.io/illustrated-word2vec/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Mechanics of Seq2seq Models With Attention</span> <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The Illustrated Transformer</span> <a href="https://jalammar.github.io/illustrated-transformer/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The Annotated Transformer</span> <a href="http://nlp.seas.harvard.edu/annotated-transformer/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Visualizing Transformer Language Models</span> <a href="https://jalammar.github.io/illustrated-gpt2/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The State of Transfer Learning in NLP</span> <a href="https://www.ruder.io/state-of-transfer-learning-in-nlp/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The Illustrated BERT, ELMo, and co.</span> <a href="https://jalammar.github.io/illustrated-bert/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">A Visual Guide to Using BERT</span> <a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Various BERT Pre-Training Methods</span> <a href="https://medium.com/analytics-vidhya/an-overview-of-the-various-bert-pre-training-methods-c365512342d8">[Link]</a>
</pre>

  
